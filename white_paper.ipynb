{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanchitgarg2204/Encoding-Candlesticks-As-Images-For-Pattern-Classification-Using-CNN/blob/main/white_paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d1fZIHA8vybV"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "\n",
        "def load_pkl(pkl_name):\n",
        "    '''\n",
        "    Args:\n",
        "        pkl_name (string): path for pickle.\n",
        "\n",
        "    Returns:\n",
        "        (dict): including following structure\n",
        "            `raw time-series data` (N, 32, 4):\n",
        "                'train_data', 'val_data', 'test_data'\n",
        "            `gasf data` (N, 32, 32, 4):\n",
        "                'train_gaf', 'val_gaf', 'test_gaf'\n",
        "            `label data` (N, 3):\n",
        "                'train_label', 'val_label', 'test_label',\n",
        "            `one-hot label data` (N, 9):\n",
        "                'train_label_arr', 'val_label_arr', 'test_label_arr'\n",
        "    '''\n",
        "    # load data from data folder\n",
        "    with open(pkl_name, 'rb') as f:\n",
        "        data = pickle.load(f)\n",
        "    return data\n",
        "\n",
        "\n",
        "def ts2gasf(ts, max_v, min_v):\n",
        "    '''\n",
        "    Args:\n",
        "        ts (numpy): (N, )\n",
        "        max_v (int): max value for normalization\n",
        "        min_v (int): min value for normalization\n",
        "\n",
        "    Returns:\n",
        "        gaf_m (numpy): (N, N)\n",
        "    '''\n",
        "    # Normalization : 0 ~ 1\n",
        "    if max_v == min_v:\n",
        "        gaf_m = np.zeros((len(ts), len(ts)))\n",
        "    else:\n",
        "        ts_nor = np.array((ts-min_v) / (max_v-min_v))\n",
        "        # Arccos\n",
        "        ts_nor_arc = np.arccos(ts_nor)\n",
        "        # GAF\n",
        "        gaf_m = np.zeros((len(ts_nor), len(ts_nor)))\n",
        "        for r in range(len(ts_nor)):\n",
        "            for c in range(len(ts_nor)):\n",
        "                gaf_m[r, c] = np.cos(ts_nor_arc[r] + ts_nor_arc[c])\n",
        "    return gaf_m\n",
        "\n",
        "\n",
        "def get_gasf(arr):\n",
        "    '''Convert time-series to gasf\n",
        "    Args:\n",
        "        arr (numpy): (N, ts_n, 4)\n",
        "\n",
        "    Returns:\n",
        "        gasf (numpy): (N, ts_n, ts_n, 4)\n",
        "\n",
        "    Todos:\n",
        "        add normalization together version\n",
        "    '''\n",
        "    arr = arr.copy()\n",
        "    gasf = np.zeros((arr.shape[0], arr.shape[1], arr.shape[1], arr.shape[2]))\n",
        "    for i in range(arr.shape[0]):\n",
        "        for c in range(arr.shape[2]):\n",
        "            each_channel = arr[i, :, c]\n",
        "            c_max = np.amax(each_channel)\n",
        "            c_min = np.amin(each_channel)\n",
        "            each_gasf = ts2gasf(each_channel, max_v=c_max, min_v=c_min)\n",
        "            gasf[i, :, :, c] = each_gasf\n",
        "    return gasf\n",
        "\n",
        "\n",
        "def gasf2ts(arr):\n",
        "    '''\n",
        "    Args:\n",
        "        arr (numpy array):  (32, 32)\n",
        "    Returns:\n",
        "        numpy.series: (1d)\n",
        "    '''\n",
        "    # Get element from diagonal\n",
        "    diag_v = np.zeros((arr.shape[0],))\n",
        "    for i in range(arr.shape[0]):\n",
        "        diag_v[i] = arr[i, i]\n",
        "    # Inverse to Arc\n",
        "    diag_v_arc = np.arccos(diag_v) / 2\n",
        "    # Inverse to Normalized ts\n",
        "    ts = np.cos(diag_v_arc)\n",
        "    return ts\n",
        "\n",
        "\n",
        "def ohlc2culr(ohlc):\n",
        "    '''\n",
        "    Args:\n",
        "        ohlc (numpy): (N, ts_n, 4)\n",
        "\n",
        "    Returns:\n",
        "        culr (numpy): (N, ts_n, 4)\n",
        "    '''\n",
        "    culr = np.zeros((ohlc.shape[0], ohlc.shape[1], ohlc.shape[2]))\n",
        "    culr[:, :, 0] =  ohlc[:, :, -1]\n",
        "    culr[:, :, 1] = ohlc[:, :, 1] - np.maximum(ohlc[:, :, 0], ohlc[:, :, -1])\n",
        "    culr[:, :, 2] = np.minimum(ohlc[:, :, 0], ohlc[:, :, -1]) - ohlc[:, :, 2]\n",
        "    culr[:, :, 3] = ohlc[:, :, -1] - ohlc[:, :, 0]\n",
        "    return culr\n",
        "\n",
        "\n",
        "def culr2ohlc(culr_n, culr):\n",
        "    '''\n",
        "    Args:\n",
        "        culr_n (numpy): (N, ts_n, 4)\n",
        "        culr (numpy): (N, ts_n, 4)\n",
        "\n",
        "    Returns:\n",
        "        ohlc (numpy): (N, ts_n, 4)\n",
        "    '''\n",
        "    ohlc = np.zeros((*culr_n.shape, ))\n",
        "    for i in range(culr_n.shape[0]):\n",
        "        for c in range(culr_n.shape[-1]):\n",
        "            # get min & max from data before normalized\n",
        "            each_culr = culr[i, :, c]\n",
        "            min_v = np.amin(each_culr)\n",
        "            max_v = np.amax(each_culr)\n",
        "            # inverse normalization\n",
        "            each_culr_n = culr_n[i, :, c]\n",
        "            culr_n[i, :, c] = (each_culr_n * (max_v - min_v)) + min_v\n",
        "        # convert culr to ohlc\n",
        "        ohlc[i, :, -1] = culr_n[i, :, 0]\n",
        "        ohlc[i, :, 0] = ohlc[i, :, -1] - culr_n[i, :, -1]\n",
        "        ohlc[i, :, 1] = culr_n[i, :, 1] + np.maximum(ohlc[i, :, 0], ohlc[i, :, -1])\n",
        "        ohlc[i, :, 2] = np.minimum(ohlc[i, :, 0], ohlc[i, :, -1]) - culr_n[i, :, 2]\n",
        "    return ohlc\n",
        "\n",
        "\n",
        "def get_slope(series):\n",
        "    y = series.values.reshape(-1, 1)\n",
        "    x = np.array(range(1, series.shape[0] + 1)).reshape(-1,1)\n",
        "    model = LinearRegression()\n",
        "    model.fit(x, y)\n",
        "    slope = model.coef_\n",
        "    return slope\n",
        "\n",
        "\n",
        "def get_trend(slope):\n",
        "    '''Need to run `process_data` first with slope only, then calculate by yourself.\n",
        "    25 percentile: 7.214285714286977e-05\n",
        "    '''\n",
        "    slope = np.array(slope)\n",
        "    thres = 7.214285714286977e-05\n",
        "    if (slope >= thres):\n",
        "        return 1\n",
        "    elif (slope <= -thres):\n",
        "        return -1\n",
        "    else:\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import trange, tqdm\n",
        "from scipy import stats\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "def rename(data):\n",
        "    rename_dc = {'Date': 'timestamp', 'Price':'close'}\n",
        "    data.rename(columns=rename_dc, inplace=True)\n",
        "    data.columns = [c.lower() for c in data.columns]\n",
        "    return data\n",
        "\n",
        "\n",
        "def process_data(data, slope=True):\n",
        "    '''Including calculation of CLUR, Quartiles, and cus trend\n",
        "    Args:\n",
        "        data (dataframe): csv data from assets. With column names open, high, low, close.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    if slope:\n",
        "        # process slpoe\n",
        "        data['diff'] = data['close'] - data['open']\n",
        "        data = data.query('diff != 0').reset_index(drop=True)\n",
        "        data['direction'] = np.sign(data['diff'])\n",
        "        data['ushadow_width'] = 0\n",
        "        data['lshadow_width'] = 0\n",
        "\n",
        "        for idx in trange(len(data)):\n",
        "            if data.loc[idx, 'direction'] == 1:\n",
        "                data.loc[idx, 'ushadow_width'] = data.loc[idx, 'high'] - data.loc[idx, 'close']\n",
        "                data.loc[idx, 'lshadow_width'] = data.loc[idx, 'open'] - data.loc[idx, 'low']\n",
        "            else:\n",
        "                data.loc[idx, 'ushadow_width'] = data.loc[idx, 'high'] - data.loc[idx, 'open']\n",
        "                data.loc[idx, 'lshadow_width'] = data.loc[idx, 'close'] - data.loc[idx, 'low']\n",
        "\n",
        "            if idx <= 50:\n",
        "                data.loc[idx, 'body_per'] = stats.percentileofscore(abs(data['diff']), abs(data.loc[idx,'diff']), 'rank')\n",
        "                data.loc[idx, 'upper_per'] = stats.percentileofscore(data['ushadow_width'], data.loc[idx,'ushadow_width'], 'rank')\n",
        "                data.loc[idx, 'lower_per'] = stats.percentileofscore(data['lshadow_width'], data.loc[idx,'lshadow_width'], 'rank')\n",
        "            else:\n",
        "                data.loc[idx, 'body_per'] = stats.percentileofscore(abs(data.loc[idx-50:idx, 'diff']),abs(data.loc[idx, 'diff']), 'rank')\n",
        "                data.loc[idx, 'upper_per'] = stats.percentileofscore(data.loc[idx-50:idx, 'ushadow_width'], data.loc[idx, 'ushadow_width'], 'rank')\n",
        "                data.loc[idx, 'lower_per'] = stats.percentileofscore(data.loc[idx-50:idx, 'lshadow_width'], data.loc[idx, 'lshadow_width'], 'rank')\n",
        "\n",
        "        data['slope'] = data['close'].rolling(7).apply(get_slope, raw=False)\n",
        "        data.dropna(inplace=True)\n",
        "    else:\n",
        "        # process trend\n",
        "        data['trend'] = data['slope'].rolling(1).apply(get_trend, raw=False)\n",
        "        data['previous_trend'] = data['trend'].shift(1).fillna(0)\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_evening_star(data, short_per=35, long_per=65):\n",
        "    '''Detect evening star pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting evening star')\n",
        "    temp = data[(data['previous_trend'] == 1) & (data['direction'] == 1)].index\n",
        "    data['evening'] = 0\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'body_per'] >= long_per)\n",
        "            cond2 = (data.loc[idx+1, 'body_per'] <= short_per)\n",
        "            cond3 = (data.loc[idx+2, 'direction'] == -1)\n",
        "            cond4 = (data.loc[idx+1, 'close'] + data.loc[idx+1, 'open'])/2 >= data.loc[idx, 'close']\n",
        "            cond5 = data.loc[idx+2, 'close'] <= ((data.loc[idx, 'open'] + data.loc[idx, 'close'])/2)\n",
        "            # cond6 = (data.loc[idx+2, 'body_per'] >= long_per)\n",
        "            cond7 = (data.loc[idx+2, 'open'] <= (data.loc[idx+1, 'open'] + data.loc[idx+1, 'close'])/2)\n",
        "            if cond1 & cond2 & cond3 & cond4 & cond5 & cond7:\n",
        "                data.loc[idx+2, 'evening'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_morning_star(data, short_per=35, long_per=65):\n",
        "    '''Detect morning star pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting morning star')\n",
        "    temp = data[(data['previous_trend'] == -1) & (data['direction'] == -1)].index\n",
        "    data['morning'] = 0\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'body_per'] >= long_per)\n",
        "            cond2 = (data.loc[idx+1, 'body_per'] <= short_per)\n",
        "            cond3 = (data.loc[idx+2, 'direction'] == 1)\n",
        "            # cond4 = max(data.loc[idx+1, 'close'], data.loc[idx+1, 'open']) <= data.loc[idx, 'close']\n",
        "            cond4 = (data.loc[idx+1, 'close'] + data.loc[idx+1, 'open'])/2 <= data.loc[idx, 'close']\n",
        "            cond5 = data.loc[idx+2, 'close'] >= ((data.loc[idx, 'open'] + data.loc[idx, 'close'])/2)\n",
        "            # cond6 = (data.loc[idx+2, 'body_per'] >= long_per)\n",
        "            cond7 = (data.loc[idx+2, 'open'] >= (data.loc[idx+1, 'open'] + data.loc[idx+1, 'close'])/2)\n",
        "            if cond1 & cond2 & cond3 & cond4 & cond5 & cond7:\n",
        "                data.loc[idx+2, 'morning'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_shooting_star(data, short_per=35, long_per=65):\n",
        "    '''Detect shooting star pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting shooting star')\n",
        "    data['shooting_star'] = 0\n",
        "    temp = data[(data['previous_trend'] == 1) & (data['direction'] == 1)].index\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'body_per'] >= long_per)\n",
        "            cond2 = (data.loc[idx, 'direction'] == 1)\n",
        "            cond3 = (data.loc[idx+1, 'ushadow_width'] > 2 * abs(data.loc[idx+1, 'diff']))\n",
        "            cond4 = (min(data.loc[idx+1, 'open'], data.loc[idx+1, 'close']) > ((data.loc[idx, 'close'] + data.loc[idx, 'open']) / 2))\n",
        "            cond5 = (data.loc[idx+1, 'lower_per'] <= short_per - 10)  # 25\n",
        "            cond6 = (data.loc[idx+1, 'upper_per'] >= long_per)\n",
        "            if cond1 & cond2 & cond3 & cond4 & cond5 & cond6:\n",
        "                data.loc[idx+1, 'shooting_star'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_hanging_man(data, short_per=35, long_per=65):\n",
        "    '''Detect hanging man pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting hanging man')\n",
        "    data['hanging_man'] = 0\n",
        "    temp = data[(data['previous_trend'] == 1) & (data['direction'] == 1)].index\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'lshadow_width'] > 2 * abs(data.loc[idx, 'diff']))\n",
        "            cond2 = (data.loc[idx, 'body_per'] <= short_per)\n",
        "            cond3 = (data.loc[idx, 'upper_per'] <= (short_per - 10))\n",
        "            cond4 = (data.loc[idx, 'lower_per'] >= long_per)\n",
        "            if cond1 & cond2 & cond3 & cond4:\n",
        "                data.loc[idx, 'hanging_man'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_bullish_engulfing(data, short_per=35, long_per=65):\n",
        "    '''Detect bullish engulfing pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting bullish engulfing')\n",
        "    data['bullish_engulfing'] = 0\n",
        "    temp = data[(data['previous_trend'] == -1) & (data['direction'] == -1)].index\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'direction'] == -1)\n",
        "            cond2 = (data.loc[idx, 'body_per'] >= long_per)\n",
        "            cond3 = (data.loc[idx+1, 'direction'] == 1)\n",
        "            cond4 = (data.loc[idx+1, 'close'] > data.loc[idx, 'open'])\n",
        "            cond5 = (data.loc[idx+1, 'open'] < data.loc[idx, 'close'])\n",
        "            if cond1 & cond2 & cond3 & cond4 & cond5:\n",
        "                data.loc[idx+1, 'bullish_engulfing'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_bearish_engulfing(data, short_per=35, long_per=65):\n",
        "    '''Detect bearish engulfing pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting bearish engulfing')\n",
        "    data['bearish_engulfing'] = 0\n",
        "    temp = data[(data['previous_trend'] == 1) & (data['direction'] == 1)].index\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'direction'] == 1)\n",
        "            cond2 = (data.loc[idx, 'body_per'] >= long_per)\n",
        "            cond3 = (data.loc[idx+1, 'direction'] == -1)\n",
        "            cond4 = (data.loc[idx+1, 'close'] < data.loc[idx, 'open'])\n",
        "            cond5 = (data.loc[idx+1, 'open'] > data.loc[idx, 'close'])\n",
        "            if cond1 & cond2 & cond3 & cond4 & cond5:\n",
        "                data.loc[idx+1, 'bearish_engulfing'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_hammer(data,short_per=35, long_per=65):\n",
        "    '''Detect hammer pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting hammer')\n",
        "    data['hammer'] = 0\n",
        "    temp = data[(data['previous_trend'] == -1) & (data['direction'] == -1)].index\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'lshadow_width'] > 2 * abs(data.loc[idx, 'diff']))\n",
        "            cond2 = (data.loc[idx, 'body_per'] <= short_per)\n",
        "            cond3 = (data.loc[idx, 'upper_per'] <= (short_per - 15))\n",
        "            cond4 = (data.loc[idx, 'lower_per'] >= long_per)\n",
        "            if cond1 & cond2 & cond3 & cond4:\n",
        "                data.loc[idx, 'hammer'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_inverted_hammer(data, short_per=35, long_per=65):\n",
        "    '''Detect inverted hammer pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting inverted hammer')\n",
        "    data['inverted_hammer'] = 0\n",
        "    temp = data[(data['previous_trend'] == -1) & (data['direction'] == -1)].index\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'direction'] == -1)\n",
        "            cond2 = (data.loc[idx, 'body_per'] >= long_per)\n",
        "            cond3 = (data.loc[idx+1, 'ushadow_width'] > 2 * abs(data.loc[idx+1, 'diff']))\n",
        "            cond4 = (max(data.loc[idx+1, 'open'], data.loc[idx+1, 'close']) < ((data.loc[idx, 'close'] + data.loc[idx, 'open']) / 2))\n",
        "            cond5 = (data.loc[idx+1, 'lower_per'] <= short_per)\n",
        "            cond6 = (data.loc[idx+1, 'upper_per'] >= long_per)\n",
        "            if cond1 & cond2 & cond3 & cond4 & cond5 & cond6:\n",
        "                data.loc[idx+1, 'inverted_hammer'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_bullish_harami(data, short_per=35, long_per=65):\n",
        "    '''Detect inverted bullish harami pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting bullish harami')\n",
        "    data['bullish_harami'] = 0\n",
        "    temp = data[(data['previous_trend'] == -1) & (data['direction'] == -1)].index\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'direction'] == -1)\n",
        "            cond2 = (data.loc[idx, 'body_per'] >= long_per)\n",
        "            cond3 = (data.loc[idx+1, 'direction'] == 1)\n",
        "            cond4 = (data.loc[idx+1, 'close'] >= ((data.loc[idx, 'open'] + data.loc[idx, 'close'])/2))\n",
        "            cond5 = (data.loc[idx+1, 'close'] < data.loc[idx, 'open'])\n",
        "            cond6 = (data.loc[idx+1, 'open'] > data.loc[idx, 'close'])\n",
        "            cond7 = (data.loc[idx+1, 'open'] <= ((data.loc[idx, 'open'] + data.loc[idx, 'close'])/2))\n",
        "            cond8 = (data.loc[idx+1, 'body_per'] >= long_per)\n",
        "            if cond1 & cond2 & cond3 & cond4 & cond5 & cond6 & cond7 & cond8:\n",
        "                data.loc[idx+1, 'bullish_harami'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_bearish_harami(data, short_per=35, long_per=65):\n",
        "    '''Detect inverted bearish harami pattern\n",
        "    Args:\n",
        "        short_per (int): percentile for determination.\n",
        "        long_per (int): percentile for determination.\n",
        "\n",
        "    Returns:\n",
        "        dataframe.\n",
        "    '''\n",
        "    print('[ Info ] : detecting bearish harami')\n",
        "    data['bearish_harami'] = 0\n",
        "    temp = data[(data['previous_trend'] == 1) & (data['direction'] == 1)].index\n",
        "    try:\n",
        "        for idx in tqdm(temp):\n",
        "            cond1 = (data.loc[idx, 'direction'] == 1)\n",
        "            cond2 = (data.loc[idx, 'body_per'] >= long_per)\n",
        "            cond3 = (data.loc[idx+1, 'direction'] == -1)\n",
        "            cond4 = (data.loc[idx+1, 'close'] <= ((data.loc[idx, 'open'] + data.loc[idx, 'close'])/2))\n",
        "            cond5 = (data.loc[idx+1, 'close'] > data.loc[idx, 'open'])\n",
        "            cond6 = (data.loc[idx+1, 'open'] < data.loc[idx, 'close'])\n",
        "            cond7 = (data.loc[idx+1, 'open'] >= ((data.loc[idx, 'open'] + data.loc[idx, 'close'])/2))\n",
        "            cond8 = (data.loc[idx+1, 'body_per'] >= long_per)\n",
        "            if cond1 & cond2 & cond3 & cond4 & cond5 & cond6 & cond7 & cond8:\n",
        "                data.loc[idx+1, 'bearish_harami'] = 1\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def detect_all(data, tasks_ls=None):\n",
        "    '''\n",
        "    Args:\n",
        "        data (dataframe): csv data after `process_data` function.\n",
        "        multi (bool): use multiprocessing or not.\n",
        "        pro_num (int): how many processes to be used.\n",
        "\n",
        "    Returns:\n",
        "        data (dataframe): dataframe with detections.\n",
        "    '''\n",
        "\n",
        "    data = detect_evening_star(data)\n",
        "    data = detect_morning_star(data)\n",
        "    data = detect_shooting_star(data)\n",
        "    data = detect_hanging_man(data)\n",
        "    data = detect_bullish_engulfing(data)\n",
        "    data = detect_bearish_engulfing(data)\n",
        "    data = detect_hammer(data)\n",
        "    data = detect_inverted_hammer(data)\n",
        "    data = detect_bullish_harami(data)\n",
        "    data = detect_bearish_harami(data)\n",
        "    return data\n",
        "\n",
        "\n",
        "def detection_result(data):\n",
        "    '''Print numbers of detection\n",
        "    Args:\n",
        "        data (dataframe): csv data after `process_data` function.\n",
        "\n",
        "    Returns:\n",
        "        data (dataframe): dataframe with detections.\n",
        "    '''\n",
        "    print('\\n[ Info ] : number of evening star is %s' % np.sum(data['evening']))\n",
        "    print('[ Info ] : number of morning star is %s' % np.sum(data['morning']))\n",
        "    print('[ Info ] : number of shooting star is %s' % np.sum(data['shooting_star']))\n",
        "    print('[ Info ] : number of hanging man is %s' % np.sum(data['hanging_man']))\n",
        "    print('[ Info ] : number of bullish engulfing is %s' % np.sum(data['bullish_engulfing']))\n",
        "    print('[ Info ] : number of bearish engulfing is %s' % np.sum(data['bearish_engulfing']))\n",
        "    print('[ Info ] : number of hammer is %s' % np.sum(data['hammer']))\n",
        "    print('[ Info ] : number of inverted hammer is %s' % np.sum(data['inverted_hammer']))\n",
        "    print('[ Info ] : number of bullish harami is %s' % np.sum(data['bullish_harami']))\n",
        "    print('[ Info ] : number of bearish harami is %s' % np.sum(data['bearish_harami']))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    TASLS_LS = [detect_evening_star, detect_morning_star, detect_shooting_star,\n",
        "                detect_hanging_man, detect_bullish_engulfing, detect_bearish_engulfing,\n",
        "                detect_hammer, detect_inverted_hammer, detect_bullish_harami,\n",
        "                detect_bearish_harami]\n",
        "\n"
      ],
      "metadata": {
        "id": "gGpuLb7ev2oP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import sys\n",
        "drive.mount('/content/drive/', force_remount = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl_xbOx7xJEx",
        "outputId": "5ad0b621-49a4-448f-dff0-c8c713c11ca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_excel('/content/drive/MyDrive/whitepaper/eur-usd-2010-17.xlsx', header=0)\n",
        "df.to_csv('data1.csv', index=False)\n",
        "data=pd.read_csv('data1.csv')"
      ],
      "metadata": {
        "id": "Os9jqevKxMDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=rename(data)\n",
        "data.head()\n",
        "# calculate features & slope\n",
        "data = process_data(data, slope=True)\n",
        "\n",
        "# calculate trend (depend on slopes)\n",
        "data = process_data(data, slope=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1HlBoB0xVKF",
        "outputId": "10ccc5f3-b7c0-44df-a17e-f307d9ea6255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2064/2064 [00:05<00:00, 382.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "detect_data = detect_all(data, TASLS_LS)\n",
        "#data.to_csv('./data/eurusd_2010_2017_patterns.csv', index=False)\n",
        "detection_result(detect_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8vkhZk_xWFk",
        "outputId": "4f2353f4-188b-433d-c45a-05a3a5fe8922"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting evening star\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 522/522 [00:00<00:00, 6913.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting morning star\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 489/490 [00:00<00:00, 6623.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting shooting star\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 522/522 [00:00<00:00, 8315.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting hanging man\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 522/522 [00:00<00:00, 7803.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting bullish engulfing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 489/490 [00:00<00:00, 10285.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting bearish engulfing\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 522/522 [00:00<00:00, 9908.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting hammer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 490/490 [00:00<00:00, 13391.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting inverted hammer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 489/490 [00:00<00:00, 7635.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting bullish harami\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 489/490 [00:00<00:00, 6065.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ Info ] : detecting bearish harami\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 522/522 [00:00<00:00, 6350.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[ Info ] : number of evening star is 0\n",
            "[ Info ] : number of morning star is 0\n",
            "[ Info ] : number of shooting star is 0\n",
            "[ Info ] : number of hanging man is 20\n",
            "[ Info ] : number of bullish engulfing is 8\n",
            "[ Info ] : number of bearish engulfing is 1\n",
            "[ Info ] : number of hammer is 6\n",
            "[ Info ] : number of inverted hammer is 0\n",
            "[ Info ] : number of bullish harami is 6\n",
            "[ Info ] : number of bearish harami is 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "from keras import backend as K\n",
        "from keras import optimizers\n",
        "import keras.utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Flatten, Conv2D, Activation\n",
        "\n",
        "\n",
        "\n",
        "def get_model(params):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Conv1\n",
        "    model.add(Conv2D(16, (2, 2), input_shape=(10, 10, 4), padding='same', strides=(1, 1)))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    # Conv2\n",
        "    model.add(Conv2D(16, (2, 2), padding='same', strides=(1, 1)))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    # FC\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    model.add(Dense(params['classes']))\n",
        "    model.add(Activation('softmax'))\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(params, data):\n",
        "    model = get_model(params)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=params['optimizer'], metrics=['accuracy'])\n",
        "    hist = model.fit(x=data['train_gaf'], y=data['train_label_arr'],\n",
        "                     validation_data=(data['val_gaf'], data['val_label_arr']),\n",
        "                     batch_size=params['batch_size'], epochs=params['epochs'], verbose=2)\n",
        "\n",
        "    return (model, hist)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    PARAMS = {}\n",
        "    PARAMS['pkl_name'] = '/content/drive/MyDrive/whitepaper/label8_eurusd_10bar_1500_500_val200_gaf_culr.pkl'\n",
        "    PARAMS['model_name'] = 'cnn_model_10bar.h5'\n",
        "    PARAMS['classes'] = 9\n",
        "    PARAMS['learning_rate'] = 0.01\n",
        "    PARAMS['epochs'] = 50\n",
        "    PARAMS['batch_size'] = 64\n",
        "    PARAMS['optimizer'] = optimizers.SGD(lr=PARAMS['learning_rate'])\n",
        "\n",
        "    # ---------------------------------------------------------\n",
        "    # load data & keras model\n",
        "\n",
        "    data =load_pkl(PARAMS['pkl_name'])\n",
        "\n",
        "    # train cnn model\n",
        "    model, hist = train_model(PARAMS, data)\n",
        "    model.save(PARAMS['model_name'])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HRtWwcdxdLf",
        "outputId": "90c7a433-ea64-4de3-c9f4-7b4ca733adda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 10, 10, 16)        272       \n",
            "                                                                 \n",
            " activation (Activation)     (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 10, 10, 16)        1040      \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 10, 10, 16)        0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 1600)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               204928    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 9)                 1161      \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 9)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 207401 (810.16 KB)\n",
            "Trainable params: 207401 (810.16 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "235/235 - 4s - loss: 2.1722 - accuracy: 0.1981 - val_loss: 2.1794 - val_accuracy: 0.2000 - 4s/epoch - 16ms/step\n",
            "Epoch 2/50\n",
            "235/235 - 4s - loss: 2.1535 - accuracy: 0.1994 - val_loss: 2.1333 - val_accuracy: 0.2000 - 4s/epoch - 16ms/step\n",
            "Epoch 3/50\n",
            "235/235 - 3s - loss: 2.1305 - accuracy: 0.2011 - val_loss: 2.1278 - val_accuracy: 0.2925 - 3s/epoch - 11ms/step\n",
            "Epoch 4/50\n",
            "235/235 - 2s - loss: 2.0873 - accuracy: 0.2047 - val_loss: 2.0563 - val_accuracy: 0.2130 - 2s/epoch - 9ms/step\n",
            "Epoch 5/50\n",
            "235/235 - 2s - loss: 2.0076 - accuracy: 0.2205 - val_loss: 1.9522 - val_accuracy: 0.2000 - 2s/epoch - 9ms/step\n",
            "Epoch 6/50\n",
            "235/235 - 2s - loss: 1.8319 - accuracy: 0.3111 - val_loss: 1.7202 - val_accuracy: 0.2880 - 2s/epoch - 9ms/step\n",
            "Epoch 7/50\n",
            "235/235 - 2s - loss: 1.5561 - accuracy: 0.4671 - val_loss: 1.4083 - val_accuracy: 0.5215 - 2s/epoch - 10ms/step\n",
            "Epoch 8/50\n",
            "235/235 - 3s - loss: 1.2998 - accuracy: 0.5687 - val_loss: 1.1676 - val_accuracy: 0.6520 - 3s/epoch - 14ms/step\n",
            "Epoch 9/50\n",
            "235/235 - 2s - loss: 1.1195 - accuracy: 0.6256 - val_loss: 1.0464 - val_accuracy: 0.5910 - 2s/epoch - 9ms/step\n",
            "Epoch 10/50\n",
            "235/235 - 2s - loss: 0.9972 - accuracy: 0.6635 - val_loss: 0.9564 - val_accuracy: 0.6805 - 2s/epoch - 10ms/step\n",
            "Epoch 11/50\n",
            "235/235 - 2s - loss: 0.9169 - accuracy: 0.6827 - val_loss: 0.8758 - val_accuracy: 0.7030 - 2s/epoch - 10ms/step\n",
            "Epoch 12/50\n",
            "235/235 - 3s - loss: 0.8601 - accuracy: 0.6971 - val_loss: 0.8093 - val_accuracy: 0.7045 - 3s/epoch - 11ms/step\n",
            "Epoch 13/50\n",
            "235/235 - 3s - loss: 0.8186 - accuracy: 0.7121 - val_loss: 0.7667 - val_accuracy: 0.7270 - 3s/epoch - 14ms/step\n",
            "Epoch 14/50\n",
            "235/235 - 3s - loss: 0.7862 - accuracy: 0.7231 - val_loss: 0.7574 - val_accuracy: 0.7405 - 3s/epoch - 11ms/step\n",
            "Epoch 15/50\n",
            "235/235 - 2s - loss: 0.7570 - accuracy: 0.7336 - val_loss: 0.7249 - val_accuracy: 0.7505 - 2s/epoch - 9ms/step\n",
            "Epoch 16/50\n",
            "235/235 - 2s - loss: 0.7306 - accuracy: 0.7457 - val_loss: 0.7010 - val_accuracy: 0.7580 - 2s/epoch - 9ms/step\n",
            "Epoch 17/50\n",
            "235/235 - 2s - loss: 0.7099 - accuracy: 0.7510 - val_loss: 0.6758 - val_accuracy: 0.7655 - 2s/epoch - 9ms/step\n",
            "Epoch 18/50\n",
            "235/235 - 2s - loss: 0.6843 - accuracy: 0.7627 - val_loss: 0.6694 - val_accuracy: 0.7585 - 2s/epoch - 11ms/step\n",
            "Epoch 19/50\n",
            "235/235 - 3s - loss: 0.6630 - accuracy: 0.7696 - val_loss: 0.6257 - val_accuracy: 0.7830 - 3s/epoch - 15ms/step\n",
            "Epoch 20/50\n",
            "235/235 - 2s - loss: 0.6428 - accuracy: 0.7745 - val_loss: 0.5917 - val_accuracy: 0.7950 - 2s/epoch - 9ms/step\n",
            "Epoch 21/50\n",
            "235/235 - 2s - loss: 0.6242 - accuracy: 0.7847 - val_loss: 0.5883 - val_accuracy: 0.8030 - 2s/epoch - 9ms/step\n",
            "Epoch 22/50\n",
            "235/235 - 2s - loss: 0.6079 - accuracy: 0.7903 - val_loss: 0.5607 - val_accuracy: 0.8050 - 2s/epoch - 9ms/step\n",
            "Epoch 23/50\n",
            "235/235 - 2s - loss: 0.5916 - accuracy: 0.7973 - val_loss: 0.5835 - val_accuracy: 0.7925 - 2s/epoch - 9ms/step\n",
            "Epoch 24/50\n",
            "235/235 - 3s - loss: 0.5792 - accuracy: 0.7967 - val_loss: 0.5271 - val_accuracy: 0.8245 - 3s/epoch - 14ms/step\n",
            "Epoch 25/50\n",
            "235/235 - 3s - loss: 0.5665 - accuracy: 0.8006 - val_loss: 0.5712 - val_accuracy: 0.7935 - 3s/epoch - 11ms/step\n",
            "Epoch 26/50\n",
            "235/235 - 2s - loss: 0.5572 - accuracy: 0.8068 - val_loss: 0.4970 - val_accuracy: 0.8415 - 2s/epoch - 9ms/step\n",
            "Epoch 27/50\n",
            "235/235 - 2s - loss: 0.5465 - accuracy: 0.8067 - val_loss: 0.4935 - val_accuracy: 0.8270 - 2s/epoch - 9ms/step\n",
            "Epoch 28/50\n",
            "235/235 - 2s - loss: 0.5359 - accuracy: 0.8137 - val_loss: 0.4874 - val_accuracy: 0.8415 - 2s/epoch - 9ms/step\n",
            "Epoch 29/50\n",
            "235/235 - 3s - loss: 0.5265 - accuracy: 0.8153 - val_loss: 0.4817 - val_accuracy: 0.8350 - 3s/epoch - 11ms/step\n",
            "Epoch 30/50\n",
            "235/235 - 3s - loss: 0.5187 - accuracy: 0.8175 - val_loss: 0.4976 - val_accuracy: 0.8290 - 3s/epoch - 14ms/step\n",
            "Epoch 31/50\n",
            "235/235 - 2s - loss: 0.5131 - accuracy: 0.8203 - val_loss: 0.4750 - val_accuracy: 0.8455 - 2s/epoch - 10ms/step\n",
            "Epoch 32/50\n",
            "235/235 - 2s - loss: 0.5067 - accuracy: 0.8208 - val_loss: 0.4647 - val_accuracy: 0.8535 - 2s/epoch - 10ms/step\n",
            "Epoch 33/50\n",
            "235/235 - 2s - loss: 0.5001 - accuracy: 0.8222 - val_loss: 0.4552 - val_accuracy: 0.8480 - 2s/epoch - 10ms/step\n",
            "Epoch 34/50\n",
            "235/235 - 2s - loss: 0.4976 - accuracy: 0.8231 - val_loss: 0.4529 - val_accuracy: 0.8450 - 2s/epoch - 9ms/step\n",
            "Epoch 35/50\n",
            "235/235 - 3s - loss: 0.4880 - accuracy: 0.8264 - val_loss: 0.4373 - val_accuracy: 0.8535 - 3s/epoch - 14ms/step\n",
            "Epoch 36/50\n",
            "235/235 - 2s - loss: 0.4838 - accuracy: 0.8260 - val_loss: 0.4525 - val_accuracy: 0.8450 - 2s/epoch - 11ms/step\n",
            "Epoch 37/50\n",
            "235/235 - 2s - loss: 0.4814 - accuracy: 0.8293 - val_loss: 0.4479 - val_accuracy: 0.8425 - 2s/epoch - 9ms/step\n",
            "Epoch 38/50\n",
            "235/235 - 2s - loss: 0.4754 - accuracy: 0.8285 - val_loss: 0.4187 - val_accuracy: 0.8595 - 2s/epoch - 9ms/step\n",
            "Epoch 39/50\n",
            "235/235 - 2s - loss: 0.4721 - accuracy: 0.8305 - val_loss: 0.4236 - val_accuracy: 0.8595 - 2s/epoch - 9ms/step\n",
            "Epoch 40/50\n",
            "235/235 - 2s - loss: 0.4673 - accuracy: 0.8333 - val_loss: 0.4550 - val_accuracy: 0.8395 - 2s/epoch - 10ms/step\n",
            "Epoch 41/50\n",
            "235/235 - 3s - loss: 0.4632 - accuracy: 0.8336 - val_loss: 0.4533 - val_accuracy: 0.8385 - 3s/epoch - 15ms/step\n",
            "Epoch 42/50\n",
            "235/235 - 2s - loss: 0.4621 - accuracy: 0.8351 - val_loss: 0.4089 - val_accuracy: 0.8600 - 2s/epoch - 9ms/step\n",
            "Epoch 43/50\n",
            "235/235 - 2s - loss: 0.4569 - accuracy: 0.8373 - val_loss: 0.4273 - val_accuracy: 0.8530 - 2s/epoch - 9ms/step\n",
            "Epoch 44/50\n",
            "235/235 - 2s - loss: 0.4559 - accuracy: 0.8364 - val_loss: 0.4124 - val_accuracy: 0.8650 - 2s/epoch - 10ms/step\n",
            "Epoch 45/50\n",
            "235/235 - 2s - loss: 0.4510 - accuracy: 0.8375 - val_loss: 0.4406 - val_accuracy: 0.8470 - 2s/epoch - 9ms/step\n",
            "Epoch 46/50\n",
            "235/235 - 3s - loss: 0.4501 - accuracy: 0.8387 - val_loss: 0.5196 - val_accuracy: 0.8005 - 3s/epoch - 13ms/step\n",
            "Epoch 47/50\n",
            "235/235 - 4s - loss: 0.4438 - accuracy: 0.8422 - val_loss: 0.4042 - val_accuracy: 0.8630 - 4s/epoch - 18ms/step\n",
            "Epoch 48/50\n",
            "235/235 - 2s - loss: 0.4403 - accuracy: 0.8421 - val_loss: 0.4044 - val_accuracy: 0.8635 - 2s/epoch - 10ms/step\n",
            "Epoch 49/50\n",
            "235/235 - 2s - loss: 0.4389 - accuracy: 0.8423 - val_loss: 0.3957 - val_accuracy: 0.8645 - 2s/epoch - 9ms/step\n",
            "Epoch 50/50\n",
            "235/235 - 2s - loss: 0.4360 - accuracy: 0.8425 - val_loss: 0.4027 - val_accuracy: 0.8620 - 2s/epoch - 9ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train & test result\n",
        "from sklearn.metrics import classification_report\n",
        "def print_result(data, model):\n",
        "    # get train & test pred-labels\n",
        "    train_pred = np.argmax(model.predict(data['train_gaf']),axis=-1)\n",
        "    #train_pred = model.predict_classes(data['train_gaf'])\n",
        "    test_pred = np.argmax(model.predict(data['test_gaf']),axis=-1)\n",
        "    #test_pred = model.predict_classes(data['test_gaf'])\n",
        "    # get train & test true-labels\n",
        "    train_label = data['train_label'][:, 0]\n",
        "    test_label = data['test_label'][:, 0]\n",
        "    # train & test confusion matrix\n",
        "    train_result_cm = confusion_matrix(train_label, train_pred, labels=range(9))\n",
        "    test_result_cm = confusion_matrix(test_label, test_pred, labels=range(9))\n",
        "\n",
        "    print(train_result_cm, '\\n', test_result_cm)\n",
        "    print()\n",
        "    print(classification_report(train_label,train_pred))\n",
        "    print()\n",
        "    print(classification_report(test_label,test_pred))\n",
        "print_result(data, model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygBmQkuc6DRx",
        "outputId": "0a3087c0-cc75-4094-d51e-66dd21ac4c29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "469/469 [==============================] - 4s 8ms/step\n",
            "157/157 [==============================] - 2s 11ms/step\n",
            "[[2098  101  170   89  141   96  117  117   71]\n",
            " [  69 1418    0   11    0    0    2    0    0]\n",
            " [  45    0 1440    0   15    0    0    0    0]\n",
            " [  58   38    0 1185    0    1    0  218    0]\n",
            " [  28    0   41    0 1344    0    4    0   83]\n",
            " [ 111    2    0    1    0 1346    1   39    0]\n",
            " [ 104    2    6    0    3    0 1321    0   64]\n",
            " [  42    6    0  102    0   12    0 1338    0]\n",
            " [  41    0    3    0  244    0   41    0 1171]] \n",
            " [[706  26  65  32  46  33  23  41  28]\n",
            " [ 17 478   0   5   0   0   0   0   0]\n",
            " [ 10   0 489   0   1   0   0   0   0]\n",
            " [ 21   8   0 405   0   0   0  66   0]\n",
            " [ 11   0  18   0 442   0   1   0  28]\n",
            " [ 41   1   0   0   0 448   0  10   0]\n",
            " [ 34   0   2   0   1   0 453   0  10]\n",
            " [ 13   1   0  15   0   5   0 466   0]\n",
            " [ 19   0   0   0  63   0  11   0 407]]\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.70      0.75      3000\n",
            "         1.0       0.90      0.95      0.92      1500\n",
            "         2.0       0.87      0.96      0.91      1500\n",
            "         3.0       0.85      0.79      0.82      1500\n",
            "         4.0       0.77      0.90      0.83      1500\n",
            "         5.0       0.93      0.90      0.91      1500\n",
            "         6.0       0.89      0.88      0.88      1500\n",
            "         7.0       0.78      0.89      0.83      1500\n",
            "         8.0       0.84      0.78      0.81      1500\n",
            "\n",
            "    accuracy                           0.84     15000\n",
            "   macro avg       0.85      0.86      0.85     15000\n",
            "weighted avg       0.85      0.84      0.84     15000\n",
            "\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.81      0.71      0.75      1000\n",
            "         1.0       0.93      0.96      0.94       500\n",
            "         2.0       0.85      0.98      0.91       500\n",
            "         3.0       0.89      0.81      0.85       500\n",
            "         4.0       0.80      0.88      0.84       500\n",
            "         5.0       0.92      0.90      0.91       500\n",
            "         6.0       0.93      0.91      0.92       500\n",
            "         7.0       0.80      0.93      0.86       500\n",
            "         8.0       0.86      0.81      0.84       500\n",
            "\n",
            "    accuracy                           0.86      5000\n",
            "   macro avg       0.87      0.88      0.87      5000\n",
            "weighted avg       0.86      0.86      0.86      5000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4gHvDaWsrWJP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}